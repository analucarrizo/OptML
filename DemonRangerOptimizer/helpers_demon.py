# -*- coding: utf-8 -*-
"""helpers_demon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ujoEFyS3s2Qhu_yXhIe04JyWios82lmU
"""

from google.colab import drive
drive.mount('/content/drive')

# !cp drive/MyDrive/OptML/DemonRangerOptimizer/optimizers.py .

from optimizers import DemonRanger
from model import *

from sklearn.model_selection import KFold
import sklearn
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import torch
import torch.nn as nn
import torchvision
from torch.nn import functional as F
import numpy as np
import time 
import itertools
from tqdm.notebook import tqdm
import torchvision.datasets as datasets
from torch.optim import Adam

torch.manual_seed(42)

def get_demon(model, config, train_loader):
  demon_optimizer = DemonRanger(params = model.parameters(),
                        lr = config["lr"],
                        weight_decay = config["wd"],
                        epochs = config["epochs"],
                        step_per_epoch = len(train_loader), 
                        betas = (0.9,0.999,0.999), # restore default AdamW betas
                        nus = (1.0,1.0), # disables QHMomentum
                        k = 0,  # disables lookahead
                        alpha = 1.0, 
                        IA = False, # enables Iterate Averaging
                        rectify = False, # disables RAdam Recitification
                        AdaMod = False, #disables AdaMod
                        AdaMod_bias_correct = False, #disables AdaMod bias corretion (not used originally)
                        use_demon = True, #enables Decaying Momentum (DEMON)
                        use_gc = False, #disables gradient centralization
                        amsgrad = False # disables amsgrad
                       )
  return demon_optimizer

def train_demon(model, train_loader, criterion, optimizer, device, view_every, IA_activate = False):
  running_loss = 0.0
        
  for i, data in enumerate(train_loader, 0):
      # get the inputs; data is a list of [inputs, labels]
      inputs, labels = data[0].to(device), data[1].to(device)

      # zero the parameter gradients
      optimizer.zero_grad()

      # forward + backward + optimize
      outputs = model(inputs)
      loss = criterion(outputs, labels)
      loss.backward()

      # val_acc = curr_acc
      optimizer.step(IA_activate)

  # print('Finished Training')

def test_demon(model, test_loader, device):
  correct = 0
  total = 0
  # since we're not training, we don't need to calculate the gradients for our outputs
  with torch.no_grad():
      for data in test_loader:
          images, labels = data[0].to(device), data[1].to(device)
          # calculate outputs by running images through the network 
          outputs = model(images).to(device)
          
          # the class with the highest energy is what we choose as prediction
          pdf = F.softmax(outputs, dim=1) 
          predicted = pdf.argmax(dim=1) 

          total += labels.size(0)
          correct += (predicted == labels).sum().item()

  # print(f'Accuracy of the network on the test images: {(100 * correct / total)} %')
  return correct/total

def hyperparameters_demon(mnist_train, model, params, device):
  start_time = time.time()
  keys, values = zip(*params.items())
  permutations_dict = [dict(zip(keys, v)) for v in itertools.product(*values)]

  opt_acc = 0.0
  opt_config = {}

  for config in tqdm(permutations_dict):
    print('-----------------------')
    print("Current Parameters: ", config)
    acc = cross_validation_demon(mnist_train, model, config, device)

    if acc > opt_acc:
      opt_acc = acc
      opt_config = config
  
  end_time = time.time()
  print(f" Best Accuracy: {opt_acc}, with parameters {opt_config}")
  print(f" Runtime: {((end_time - start_time)/60):.4} minutes")
  return opt_config, opt_acc

def cross_validation_demon(mnist_train, model, params, device, IA_activate = False):
  kfold = KFold(n_splits = params["folds"], shuffle = True)
  results = {}

  for fold, (train_idx, val_idx) in enumerate(kfold.split(mnist_train)):
    print(f"---------------Fold {fold+1}--------------")

    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)
    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)

    train_loader = torch.utils.data.DataLoader(mnist_train, 
                                               batch_size=params["batch_size"],
                                               sampler = train_subsampler,
                                               pin_memory=torch.cuda.is_available())
    val_loader = torch.utils.data.DataLoader(mnist_train, 
                                             batch_size=params["batch_size"],
                                             sampler = val_subsampler,
                                             pin_memory = torch.cuda.is_available())

    demon = get_demon(model, params, train_loader)
    model.apply(reset_weights)

    val_acc = 0.0
    for epoch in range(params["epochs"]):
      train_demon(model, train_loader, params["criterion"](), demon, device, params["view_every"])
      curr_acc = (test_demon(model, val_loader, device)) * 100.0
      val_acc = curr_acc

      print(f"epoch {epoch + 1}/{params['epochs']}, Accuracy: {curr_acc:.4}")
    
    results[fold] = curr_acc

  # Print fold results
  print('--------------------------------')
  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {params["folds"]} FOLDS')
  print('--------------------------------')
  
  sum = 0.0
  for fold, value in results.items():
    print(f"Fold {fold}: {value} %")
    sum += value
  
  avg_acc = sum/len(results.items())
  print(f"Average: {avg_acc:.4} %")
  print("-----------")
  
  return avg_acc
  
def run_best_model_demon(mnist_train, mnist_test, model, config, device):
  train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=config["batch_size"], shuffle=True, pin_memory=torch.cuda.is_available())
  test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=config["batch_size"], shuffle=True, pin_memory = torch.cuda.is_available())
  
  start_time = time.time()
  nb_epochs = config["epochs"]
  nb_folds = config["folds"]
  accs = [0]*nb_epochs

  for fold in range(nb_folds):
    model.to(device)

    batch_size = config["batch_size"]
    criterion = config["criterion"]()
    optimizer = get_demon(model, config, train_loader)
    
    print(f"------ FOLD {fold + 1} -------")
    
    for epoch in range(nb_epochs):
      train_demon(model, train_loader, criterion, optimizer, device, config["view_every"])
      acc = test_demon(model, test_loader, device)

      accs[epoch] += acc

      print(f"Epoch {epoch} - accuracy: {acc:.4}")

  for i in range(len(accs)):
    accs[i] /= nb_folds

  print(f"----- {((time.time() - start_time)/60):.4} minutes")
  return accs